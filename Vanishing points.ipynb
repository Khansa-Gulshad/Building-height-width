{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645f2d39-69df-4c70-b238-9c21ebf35bc3",
   "metadata": {},
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f00e88-c93a-471e-9ff3-3fd7b0d73fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== USER CONFIG (edit these) ====\n",
    "NEURVPS_ROOT = \"/users/project1/pt01183/Building-height-width/external/neurvps\"  # repo root that contains the 'neurvps/' package\n",
    "IN_DIR       = \"/users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq\"\n",
    "OUT_DIR      = \"/users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/my5_vpts\"\n",
    "CFG_YAML     = \"/users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/config_gsv.yaml\"\n",
    "CKPT_PATH    = \"/users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/checkpoint_state_anet.pth\"\n",
    "\n",
    "# Use \"cuda:0\" if you're inside a GPU session; otherwise \"cpu\"\n",
    "DEVICE       = \"cuda:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395d37e2-ce9f-4771-9e74-57521f4bab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurvps package path -> /users/project1/pt01183/Building-height-width/external/neurvps/neurvps/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pathlib\n",
    "\n",
    "def ensure_neurvps_on_path(repo_root: str):\n",
    "    repo_root = os.path.abspath(repo_root)\n",
    "    candidates = [repo_root, os.path.join(repo_root, \"src\")]\n",
    "    added = False\n",
    "    for c in candidates:\n",
    "        if os.path.isdir(os.path.join(c, \"neurvps\")):\n",
    "            if c not in sys.path:\n",
    "                sys.path.insert(0, c)\n",
    "                added = True\n",
    "                break\n",
    "    if not added:\n",
    "        raise RuntimeError(\n",
    "            f\"Could not find 'neurvps' package under {repo_root} (or {repo_root}/src). \"\n",
    "            \"Check NEURVPS_ROOT and repo layout.\"\n",
    "        )\n",
    "\n",
    "ensure_neurvps_on_path(NEURVPS_ROOT)\n",
    "\n",
    "# quick sanity print\n",
    "import importlib\n",
    "neurvps = importlib.import_module(\"neurvps\")\n",
    "print(\"neurvps package path ->\", getattr(neurvps, \"__file__\", \"(unknown)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1c7776-3136-42c1-b2c2-28720df05ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, pathlib\n",
    "shutil.rmtree(pathlib.Path.home()/\".cache\"/\"torch_extensions\", ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22415872-ee8d-40a0-8119-0c98f190ea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes needed: /users/project1/pt01183/Building-height-width/external/neurvps/neurvps/models/cpp/deform_conv.cpp\n",
      "No changes needed: /users/project1/pt01183/Building-height-width/external/neurvps/neurvps/models/cpp/deform_conv_cuda.cu\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "NEUR = Path(\"/users/project1/pt01183/Building-height-width/external/neurvps\")\n",
    "\n",
    "targets = [\n",
    "    NEUR/\"neurvps/models/cpp/deform_conv.cpp\",\n",
    "    NEUR/\"neurvps/models/cpp/deform_conv_cuda.cu\",\n",
    "]\n",
    "\n",
    "def patch_text(s: str) -> str:\n",
    "    t = s\n",
    "    # Deprecated .type() API → modern API\n",
    "    t = t.replace(\".type().is_cuda()\", \".is_cuda()\")\n",
    "    t = t.replace(\".type().scalarType()\", \".scalar_type()\")\n",
    "    # AT_DISPATCH...(... .type(), ...) → (... .scalar_type(), ...)\n",
    "    t = re.sub(r\"AT_DISPATCH_FLOATING_TYPES_AND_HALF\\(\\s*([A-Za-z0-9_]+)\\.type\\(\\)\",\n",
    "               r\"AT_DISPATCH_FLOATING_TYPES_AND_HALF(\\1.scalar_type()\", t)\n",
    "    t = re.sub(r\"AT_DISPATCH_FLOATING_TYPES\\(\\s*([A-Za-z0-9_]+)\\.type\\(\\)\",\n",
    "               r\"AT_DISPATCH_FLOATING_TYPES(\\1.scalar_type()\", t)\n",
    "    return t\n",
    "\n",
    "for p in targets:\n",
    "    src = p.read_text()\n",
    "    new = patch_text(src)\n",
    "    if new != src:\n",
    "        p.write_text(new)\n",
    "        print(\"Patched:\", p)\n",
    "    else:\n",
    "        print(\"No changes needed:\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64d2f840-34a4-45a0-a803-4ea395e56680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.13 | packaged by conda-forge | (main, Oct 26 2023, 18:07:37) [GCC 12.3.0]\n",
      "Mon Oct 13 16:53:37 2025       \n",
      "Torch: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA runtime (torch): 12.4\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "Tensor device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# GPU + Torch sanity\n",
    "import sys, subprocess, os\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "# show GPU/driver\n",
    "try:\n",
    "    out = subprocess.check_output([\"nvidia-smi\"], text=True)\n",
    "    print(out.splitlines()[0])\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi not available:\", e)\n",
    "\n",
    "# torch check\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA runtime (torch):\", torch.version.cuda)\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "        x = torch.rand(2,2).cuda()\n",
    "        print(\"Tensor device:\", x.device)\n",
    "except Exception as e:\n",
    "    print(\"Torch not installed or failed to import:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2cf81-a086-4084-9bab-71dd8e85dfc0",
   "metadata": {},
   "source": [
    "## Make a GSV-specific config (sets the correct focal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc06d3b-0e82-4702-b88a-845c30890157",
   "metadata": {},
   "source": [
    "Use this first. It tells NeurVPS the focal that matches your 640×640, FOV = 100° images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c924f967-9281-4ea5-8f82-7d4611135587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/config_gsv.yaml\n",
      "focal(px): 268.51188197672957\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml, math\n",
    "\n",
    "REPO   = Path(\"/users/project1/pt01183/Building-height-width\")\n",
    "NEUR   = REPO / \"external\" / \"neurvps\"\n",
    "BASE   = NEUR / \"logs\" / \"tmm17\" / \"config.yaml\"        # downloaded from HF\n",
    "CFG_GSV= NEUR / \"logs\" / \"tmm17\" / \"config_gsv.yaml\"\n",
    "\n",
    "# your GSV camera numbers (from earlier): fov=100°, width=640\n",
    "fov_deg = 100.0\n",
    "W = 640\n",
    "f_px = (W/2.0) / math.tan(math.radians(fov_deg/2.0))   # ≈ 268.51 px\n",
    "\n",
    "with open(BASE, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Ensure expected structure exists\n",
    "cfg.setdefault(\"data\", {})\n",
    "cfg[\"data\"][\"focal\"] = float(f_px)\n",
    "\n",
    "# IMPORTANT: the TMM17 checkpoint expects NO conic/DCN branch:\n",
    "cfg.setdefault(\"model\", {})\n",
    "cfg[\"model\"][\"conic_6x\"] = False   # <- critical\n",
    "# (leave the rest of model settings as-is; do not add new keys)\n",
    "\n",
    "with open(CFG_GSV, \"w\") as f:\n",
    "    yaml.safe_dump(cfg, f, sort_keys=False)\n",
    "\n",
    "print(\"Wrote:\", CFG_GSV)\n",
    "print(\"focal(px):\", f_px)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34089ba6-781a-41d4-a496-3d966901d69a",
   "metadata": {},
   "source": [
    "## Gather your 5 RGBs (already 640×640) into a small input folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41efd8f-27d0-4de7-8493-3474fdbbf13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 5 images to /users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq:\n",
      " - 6_196.jpg\n",
      " - 2_190.jpg\n",
      " - 7_4.jpg\n",
      " - 8_139.jpg\n",
      " - 9_196.jpg\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil, glob\n",
    "\n",
    "REPO = Path(\"/users/project1/pt01183/Building-height-width\")\n",
    "NEURVPS = REPO / \"external\" / \"neurvps\"\n",
    "\n",
    "# <-- your actual RGB source folder\n",
    "RAW_DIR = REPO / \"Gdańsk, Poland\" / \"save_rgb\" / \"imgs\"\n",
    "\n",
    "KEEP_IDS = [\"6_196\",\"2_190\",\"7_4\",\"8_139\",\"9_196\"]\n",
    "\n",
    "# We'll just copy to a small working folder for NeurVPS (no resize needed)\n",
    "SQ_DIR  = NEURVPS / \"data\" / \"my5_sq\"\n",
    "SQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def copy_by_id(src_dir: Path, sid: str, dst_dir: Path) -> Path | None:\n",
    "    # try common extensions; if filenames have suffixes, also try glob with sid.*\n",
    "    candidates = []\n",
    "    for ext in (\".jpg\",\".jpeg\",\".png\",\".JPG\",\".PNG\"):\n",
    "        p = src_dir / f\"{sid}{ext}\"\n",
    "        if p.exists():\n",
    "            candidates.append(p)\n",
    "    if not candidates:\n",
    "        # fallback: any file that starts with id (e.g., sid_*.jpg)\n",
    "        candidates = [Path(p) for p in glob.glob(str(src_dir / f\"{sid}.*\"))]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    src = sorted(candidates, key=lambda x: x.suffix.lower())[0]\n",
    "    dst = dst_dir / (sid + src.suffix.lower())\n",
    "    shutil.copy2(src, dst)\n",
    "    return dst\n",
    "\n",
    "copied = []\n",
    "for sid in KEEP_IDS:\n",
    "    outp = copy_by_id(RAW_DIR, sid, SQ_DIR)\n",
    "    if outp:\n",
    "        copied.append(outp.name)\n",
    "    else:\n",
    "        print(f\"[WARN] Could not find an image for ID {sid} in {RAW_DIR}\")\n",
    "\n",
    "print(f\"Copied {len(copied)} images to {SQ_DIR}:\")\n",
    "for name in copied:\n",
    "    print(\" -\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bdbcc-b764-4fab-ac0e-57d9cbd5bf57",
   "metadata": {},
   "source": [
    "## Create the runner script (one-time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f96284-ca67-4349-b987-e5759b7b4efd",
   "metadata": {},
   "source": [
    "This is a small helper we write ourselves so you can run a folder and get per-image JSONs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bd5d9d1-8279-4f75-8e4f-24224701cea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "REPO_ROOT      = \"/users/project1/pt01183/Building-height-width\"\n",
    "NEURVPS_ROOT   = f\"{REPO_ROOT}/external/neurvps\"\n",
    "\n",
    "IN_DIR         = f\"{NEURVPS_ROOT}/data/my5_sq\"\n",
    "OUT_DIR        = f\"{NEURVPS_ROOT}/logs/tmm17/my5_vpts\"\n",
    "CFG_YAML       = f\"{NEURVPS_ROOT}/logs/tmm17/config_gsv.yaml\"\n",
    "CKPT_PATH      = f\"{NEURVPS_ROOT}/logs/tmm17/checkpoint_state_anet.pth\"  # or *_anetfix.pth\n",
    "\n",
    "import torch\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "306e1df3-bebd-4add-8b5b-cdaa32b67482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, pkgutil, inspect, types, sys\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "def _wrap_backbone_output(bb: nn.Module) -> nn.Module:\n",
    "    \"\"\"Ensure backbone(x) returns a tuple/list so [...][0] works in VanishingNet.\"\"\"\n",
    "    class _Wrap(nn.Module):\n",
    "        def __init__(self, core): \n",
    "            super().__init__(); self.core = core\n",
    "        def forward(self, x):\n",
    "            y = self.core(x)\n",
    "            if isinstance(y, (list, tuple)): \n",
    "                return y\n",
    "            return (y,)  # make it indexable\n",
    "    return _Wrap(bb)\n",
    "\n",
    "def _instantiate_candidate(obj, M):\n",
    "    try:\n",
    "        # try with config first\n",
    "        return obj(M)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return obj()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def autodiscover_backbone(M):\n",
    "    \"\"\"\n",
    "    Search neurvps.models.* for anything that looks like a backbone builder\n",
    "    or an nn.Module class we can instantiate.\n",
    "    \"\"\"\n",
    "    discovered = []\n",
    "\n",
    "    # 1) Known builder modules first\n",
    "    for modname in (\"neurvps.models.builder\", \"neurvps.models.backbone\", \"neurvps.models.backbones\"):\n",
    "        try:\n",
    "            m = importlib.import_module(modname)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for name in dir(m):\n",
    "            obj = getattr(m, name, None)\n",
    "            # candidate factory functions\n",
    "            if callable(obj) and any(key in name.lower() for key in [\"build\", \"make\", \"backbone\", \"resnet\"]):\n",
    "                try:\n",
    "                    bb = _instantiate_candidate(obj, M)\n",
    "                    if isinstance(bb, nn.Module):\n",
    "                        return _wrap_backbone_output(bb)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # 2) Broad crawl under neurvps.models\n",
    "    try:\n",
    "        base = importlib.import_module(\"neurvps.models\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Cannot import neurvps.models: {e}\")\n",
    "    prefix = base.__name__ + \".\"\n",
    "    for finder, name, ispkg in pkgutil.walk_packages(base.__path__, prefix):\n",
    "        try:\n",
    "            mod = importlib.import_module(name)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for _, cls in inspect.getmembers(mod, inspect.isclass):\n",
    "            if not issubclass(cls, nn.Module):\n",
    "                continue\n",
    "            # Heuristics: likely backbone names\n",
    "            if any(key in cls.__name__.lower() for key in [\"backbone\", \"resnet\", \"vgg\", \"hrnet\", \"swin\"]):\n",
    "                bb = _instantiate_candidate(cls, M)\n",
    "                if isinstance(bb, nn.Module):\n",
    "                    return _wrap_backbone_output(bb)\n",
    "\n",
    "    return None  # not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b336529e-ea8c-4cf5-8b40-05ddbcdd0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def make_torchvision_backbone(which=\"resnet50\", pretrained=False, out_layer=\"layer3\"):\n",
    "    \"\"\"\n",
    "    Returns a backbone nn.Module that outputs a feature map (and wraps to tuple).\n",
    "    out_layer: \"layer3\" (stride 16) is a good default for dense heads.\n",
    "    \"\"\"\n",
    "    if which == \"resnet50\":\n",
    "        m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        C = 1024 if out_layer == \"layer3\" else 2048\n",
    "    elif which == \"resnet34\":\n",
    "        m = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        C = 256 if out_layer == \"layer3\" else 512\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backbone name\")\n",
    "\n",
    "    # Build a feature extractor up to desired layer\n",
    "    layers = [\n",
    "        (\"conv1\", m.conv1),\n",
    "        (\"bn1\",   m.bn1),\n",
    "        (\"relu\",  m.relu),\n",
    "        (\"maxpool\", m.maxpool),\n",
    "        (\"layer1\", m.layer1),\n",
    "        (\"layer2\", m.layer2),\n",
    "        (\"layer3\", m.layer3),\n",
    "    ]\n",
    "    if out_layer == \"layer4\":\n",
    "        layers.append((\"layer4\", m.layer4))\n",
    "\n",
    "    feat = nn.Sequential(*(getattr(nn, 'Identity')() if False else layer for name, layer in layers))\n",
    "    # Wrap so forward returns (feat,)\n",
    "    class FeatWrap(nn.Module):\n",
    "        def __init__(self, core): super().__init__(); self.core = core\n",
    "        def forward(self, x): return (self.core(x),)\n",
    "    return FeatWrap(feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c0375-e0e4-46e9-b719-988279c85dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, re, types, importlib\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "def _extract_state(raw):\n",
    "    if isinstance(raw, dict):\n",
    "        for k in (\"state_dict\",\"model_state_dict\"):\n",
    "            if k in raw and isinstance(raw[k], dict):\n",
    "                return raw[k]\n",
    "    return raw\n",
    "\n",
    "def _strip_wrappers(k: str) -> str:\n",
    "    for pref in (\"module.\", \"model.\"):\n",
    "        if k.startswith(pref): return k[len(pref):]\n",
    "    return k\n",
    "\n",
    "def _remap_state_for_model(state, model_keys):\n",
    "    has_anet_in_model = any(k.startswith(\"anet.\") for k in model_keys)\n",
    "    has_top_backbone  = any(k.startswith(\"backbone.\") for k in model_keys)\n",
    "    has_anet_backbone = any(k.startswith(\"anet.backbone.\") for k in model_keys)\n",
    "    out = {}\n",
    "    for k, v in state.items():\n",
    "        kk = _strip_wrappers(k)\n",
    "        if has_top_backbone and not has_anet_backbone and kk.startswith(\"anet.backbone.\"):\n",
    "            kk = kk[len(\"anet.\"):]\n",
    "        if has_anet_in_model and not kk.startswith(\"anet.\"):\n",
    "            if kk.startswith(\"backbone.\") or re.match(r\"^(fc[0-3]|bn[1-4]|conv[1-4]|score)(\\.|$)\", kk):\n",
    "                kk = \"anet.\" + kk\n",
    "        if not has_anet_in_model and kk.startswith(\"anet.\"):\n",
    "            kk = kk[len(\"anet.\"):]\n",
    "        out[kk] = v\n",
    "    return out\n",
    "\n",
    "def load_model_strict_yaml(cfg_path: str, ckpt_path: str, device: str):\n",
    "    # Imports\n",
    "    cfgmod = importlib.import_module(\"neurvps.config\")\n",
    "    vn_mod = importlib.import_module(\"neurvps.models.vanishing_net\")\n",
    "    VanishingNet = vn_mod.VanishingNet\n",
    "\n",
    "    # YAML\n",
    "    with open(cfg_path, \"r\") as f:\n",
    "        cfg_yaml = yaml.safe_load(f) or {}\n",
    "    data_cfg  = (cfg_yaml.get(\"data\")  or {})\n",
    "    model_cfg = (cfg_yaml.get(\"model\") or {})\n",
    "\n",
    "    # Propagate scalars + lists to M\n",
    "    for k, v in model_cfg.items():\n",
    "        if isinstance(v, (int,float,bool,str)) or k in {\"multires\",\"resolutions\"}:\n",
    "            try: setattr(cfgmod.M, k, v)\n",
    "            except Exception: pass\n",
    "\n",
    "    # Ensure essentials\n",
    "    defaults = {\n",
    "        \"fc_channel\": 1024,\n",
    "        \"multires\": [0.01,0.05,0.2,0.8],\n",
    "        \"smp_pos\": 1, \"smp_neg\": 1, \"smp_rnd\": 8, \"smp_multiplier\": 2.0\n",
    "    }\n",
    "    for k, v in defaults.items():\n",
    "        if not hasattr(cfgmod.M, k): setattr(cfgmod.M, k, v)\n",
    "\n",
    "    # Constants shim if needed (C.io.num_vpts)\n",
    "    try:\n",
    "        consts = importlib.import_module(\"neurvps.constants\")\n",
    "        if hasattr(consts, \"C\"):\n",
    "            C = consts.C\n",
    "        elif hasattr(consts, \"Constants\"):\n",
    "            C = consts.Constants()\n",
    "        else:\n",
    "            raise ImportError\n",
    "    except Exception:\n",
    "        class _IO: pass\n",
    "        class _C: pass\n",
    "        C = _C(); C.io = _IO(); C.io.num_vpts = 3\n",
    "        import sys as _sys\n",
    "        _sys.modules[\"neurvps.constants\"] = types.SimpleNamespace(C=C)\n",
    "\n",
    "    focal = float(data_cfg.get(\"focal\", 1.0))\n",
    "\n",
    "    # 1) Try to auto-discover a backbone from the repo\n",
    "    bb = autodiscover_backbone(cfgmod.M)\n",
    "\n",
    "    # 2) If still None, fallback to torchvision ResNet (pretrained=False to match your env)\n",
    "    if bb is None:\n",
    "        print(\"[info] No repo backbone found; using torchvision resnet50 (layer3).\")\n",
    "        bb = make_torchvision_backbone(which=\"resnet50\", pretrained=False, out_layer=\"layer3\")\n",
    "\n",
    "    # Build model\n",
    "    output_stride  = getattr(cfgmod.M, \"output_stride\", 32)\n",
    "    upsample_scale = getattr(cfgmod.M, \"upsample_scale\", 4)\n",
    "    model = VanishingNet(bb, output_stride, upsample_scale).to(device).eval()\n",
    "\n",
    "    # Load checkpoint\n",
    "    raw = torch.load(ckpt_path, map_location=device)\n",
    "    state = _extract_state(raw)\n",
    "    mk = list(model.state_dict().keys())\n",
    "    state = _remap_state_for_model(state, mk)\n",
    "    state_pruned = {k: v for k, v in state.items() if k in mk}\n",
    "    incompatible = model.load_state_dict(state_pruned, strict=False)\n",
    "    print(f\"[load] missing={len(getattr(incompatible,'missing_keys',[]))} \"\n",
    "          f\"unexpected={len(getattr(incompatible,'unexpected_keys',[]))}\")\n",
    "\n",
    "    # Sanity\n",
    "    assert isinstance(model.backbone, nn.Module), \"backbone is not nn.Module\"\n",
    "    return model, focal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c752a85-dfce-496e-a6c0-b7bce540aab9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not build a backbone nn.Module from the repo.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, focal \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_strict_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG_YAML\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCKPT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReady. focal =\u001b[39m\u001b[38;5;124m\"\u001b[39m, focal, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| device =\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEVICE)\n",
      "Cell \u001b[0;32mIn[18], line 133\u001b[0m, in \u001b[0;36mload_model_strict_yaml\u001b[0;34m(cfg_path, ckpt_path, device)\u001b[0m\n\u001b[1;32m    130\u001b[0m focal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(data_cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1.0\u001b[39m))\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# backbone\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m backbone \u001b[38;5;241m=\u001b[39m \u001b[43m_build_backbone_from_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfgmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# construct model properly\u001b[39;00m\n\u001b[1;32m    136\u001b[0m output_stride  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(cfgmod\u001b[38;5;241m.\u001b[39mM, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_stride\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 86\u001b[0m, in \u001b[0;36m_build_backbone_from_repo\u001b[0;34m(M)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m bb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m bb\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a backbone nn.Module from the repo.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not build a backbone nn.Module from the repo."
     ]
    }
   ],
   "source": [
    "model, focal = load_model_strict_yaml(CFG_YAML, CKPT_PATH, DEVICE)\n",
    "print(\"Ready. focal =\", focal, \"| device =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fd05e63-2cb9-4b22-9e17-04e174a43e82",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not build a backbone nn.Module from the repo.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, focal \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_strict_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG_YAML\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCKPT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReady. focal =\u001b[39m\u001b[38;5;124m\"\u001b[39m, focal, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| device =\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEVICE)\n",
      "Cell \u001b[0;32mIn[18], line 133\u001b[0m, in \u001b[0;36mload_model_strict_yaml\u001b[0;34m(cfg_path, ckpt_path, device)\u001b[0m\n\u001b[1;32m    130\u001b[0m focal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(data_cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1.0\u001b[39m))\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# backbone\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m backbone \u001b[38;5;241m=\u001b[39m \u001b[43m_build_backbone_from_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfgmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# construct model properly\u001b[39;00m\n\u001b[1;32m    136\u001b[0m output_stride  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(cfgmod\u001b[38;5;241m.\u001b[39mM, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_stride\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 86\u001b[0m, in \u001b[0;36m_build_backbone_from_repo\u001b[0;34m(M)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m bb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m bb\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a backbone nn.Module from the repo.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not build a backbone nn.Module from the repo."
     ]
    }
   ],
   "source": [
    "model, focal = load_model_strict_yaml(CFG_YAML, CKPT_PATH, DEVICE)\n",
    "print(\"Ready. focal =\", focal, \"| device =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e57994ec-c9b7-48aa-9759-dc610da0de84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files in /users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00, 59.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[error] failed on /users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq/2_190.jpg: 'Box' object is not callable\n",
      "[error] failed on /users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq/6_196.jpg: 'Box' object is not callable\n",
      "[error] failed on /users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq/7_4.jpg: 'Box' object is not callable\n",
      "[error] failed on /users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq/8_139.jpg: 'Box' object is not callable\n",
      "[error] failed on /users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq/9_196.jpg: 'Box' object is not callable\n",
      "Done. JSONs: /users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/my5_vpts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_folder(model, focal, device, IN_DIR, OUT_DIR)\n",
    "print(\"Done. JSONs:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7475f67f-00ee-4241-9493-588b0fc85c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanishingNet.forward signature: (self, input_dict)\n",
      "\n",
      "--- forward source (first ~80 lines) ---\n",
      "    def forward(self, input_dict):\n",
      "        x = self.backbone(input_dict[\"image\"])[0]\n",
      "        N, _, H, W = x.shape\n",
      "        test = input_dict.get(\"test\", False)\n",
      "        if test:\n",
      "            c = len(input_dict[\"vpts\"])\n",
      "        else:\n",
      "            c = M.smp_rnd + C.io.num_vpts * len(M.multires) * (M.smp_pos + M.smp_neg)\n",
      "        x = x[:, None].repeat(1, c, 1, 1, 1).reshape(N * c, _, H, W)\n",
      "\n",
      "        if test:\n",
      "            vpts = [to_pixel(v) for v in input_dict[\"vpts\"]]\n",
      "            vpts = torch.tensor(vpts, device=x.device)\n",
      "            return self.anet(x, vpts).sigmoid()\n",
      "\n",
      "        vpts_gt = input_dict[\"vpts\"].cpu().numpy()\n",
      "        vpts, y = [], []\n",
      "        for n in range(N):\n",
      "\n",
      "            def add_sample(p):\n",
      "                vpts.append(to_pixel(p))\n",
      "                y.append(to_label(p, vpts_gt[n]))\n",
      "\n",
      "            for vgt in vpts_gt[n]:\n",
      "                for st, ed in zip([0] + M.multires[:-1], M.multires):\n",
      "                    # positive samples\n",
      "                    for _ in range(M.smp_pos):\n",
      "                        add_sample(sample_sphere(vgt, st, ed))\n",
      "                    # negative samples\n",
      "                    for _ in range(M.smp_neg):\n",
      "                        add_sample(sample_sphere(vgt, ed, ed * M.smp_multiplier))\n",
      "            # random samples\n",
      "            for _ in range(M.smp_rnd):\n",
      "                add_sample(sample_sphere(np.array([0, 0, 1]), 0, math.pi / 2))\n",
      "\n",
      "        y = torch.tensor(y, device=x.device, dtype=torch.float)\n",
      "        vpts = torch.tensor(vpts, device=x.device)\n",
      "\n",
      "        x = self.anet(x, vpts)\n",
      "        L = self.loss(x, y)\n",
      "        maskn = (y == 0).float()\n",
      "        maskp = (y == 1).float()\n",
      "        losses = {}\n",
      "        for i in range(len(M.multires)):\n",
      "            assert maskn[:, i].sum().item() != 0\n",
      "            assert maskp[:, i].sum().item() != 0\n",
      "            losses[f\"lneg{i}\"] = (L[:, i] * maskn[:, i]).sum() / maskn[:, i].sum()\n",
      "            losses[f\"lpos{i}\"] = (L[:, i] * maskp[:, i]).sum() / maskp[:, i].sum()\n",
      "\n",
      "        return {\n",
      "            \"losses\": [losses],\n",
      "            \"preds\": {\"vpts\": vpts, \"scores\": x.sigmoid(), \"ys\": y},\n",
      "        }\n"
     ]
    }
   ],
   "source": [
    "import inspect, neurvps\n",
    "from neurvps.models import vanishing_net as vn\n",
    "\n",
    "print(\"VanishingNet.forward signature:\", inspect.signature(vn.VanishingNet.forward))\n",
    "print(\"\\n--- forward source (first ~80 lines) ---\")\n",
    "src = inspect.getsource(vn.VanishingNet.forward)\n",
    "print(\"\\n\".join(src.splitlines()[:80]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f67de-2e5a-4275-a937-178c08d3ba77",
   "metadata": {},
   "source": [
    "## Run NeurVPS on the folder and get JSON VPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fdbeca-09d3-4a3f-981d-6ea97c0d1d24",
   "metadata": {},
   "source": [
    "Convert the checkpoint to a pure state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12b08ecc-173d-4a3e-88b4-1d7b3c354659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples (old -> new):\n",
      "  backbone.conv1.weight  ->  anet.backbone.conv1.weight\n",
      "  backbone.conv1.bias  ->  anet.backbone.conv1.bias\n",
      "  backbone.bn1.weight  ->  anet.backbone.bn1.weight\n",
      "  backbone.bn1.bias  ->  anet.backbone.bn1.bias\n",
      "  backbone.bn1.running_mean  ->  anet.backbone.bn1.running_mean\n",
      "Wrote: /users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/checkpoint_state_anetfix.pth num_keys: 413\n"
     ]
    }
   ],
   "source": [
    "import torch, re\n",
    "from pathlib import Path\n",
    "\n",
    "SRC = Path(\"/users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/checkpoint_state_vps.pth\")\n",
    "DST = SRC.with_name(\"checkpoint_state_anetfix.pth\")\n",
    "\n",
    "sd = torch.load(SRC, map_location=\"cpu\")\n",
    "new_sd = {}\n",
    "\n",
    "for k, v in sd.items():\n",
    "    nk = k\n",
    "    # If backbone.* -> anet.backbone.*\n",
    "    if nk.startswith(\"backbone.\"):\n",
    "        nk = \"anet.\" + nk\n",
    "    # If it's one of the A-Net head blocks -> add anet.* in front\n",
    "    elif re.match(r\"^(fc[0-3]|bn[1-4]|conv[1-4])(\\.|$)\", nk):\n",
    "        nk = \"anet.\" + nk\n",
    "    # else: leave as-is\n",
    "    new_sd[nk] = v\n",
    "\n",
    "print(\"Examples (old -> new):\")\n",
    "for i, (ok, ov) in enumerate(sd.items()):\n",
    "    if i >= 5: break\n",
    "    nk = ok\n",
    "    if nk.startswith(\"backbone.\"):\n",
    "        nk = \"anet.\" + nk\n",
    "    elif re.match(r\"^(fc[0-3]|bn[1-4]|conv[1-4])(\\.|$)\", nk):\n",
    "        nk = \"anet.\" + nk\n",
    "    print(f\"  {ok}  ->  {nk}\")\n",
    "\n",
    "torch.save(new_sd, DST)\n",
    "print(\"Wrote:\", DST, \"num_keys:\", len(new_sd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df4e53c5-3005-4119-a028-48fc90fe064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has anet?: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sd = torch.load(\"/users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/checkpoint_state_anetfix.pth\", map_location=\"cpu\")\n",
    "print(\"has anet?:\", any(k.startswith(\"anet.\") for k in sd.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c32c8567-0be1-4850-abdc-aad4a864f2f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:\n",
      "  /mnt/host_scratch/envs/new_geospatial_env/bin/python /users/project1/pt01183/Building-height-width/external/neurvps/misc/run_on_folder.py /users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq /users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/my5_vpts /users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/config_gsv.yaml /users/project1/pt01183/Building-height-width/external/neurvps/logs/tmm17/checkpoint_state_anetfix.pth cuda:0\n",
      "[debug] backbone type: <class 'dict'>\n",
      "[warn] backbone is not an nn.Module; the builder path above likely didn’t trigger. State_dict will not match until backbone is a module.\n",
      "[state_dict] loaded with missing=0 unexpected=377\n",
      "  unexpected (first 8): ['anet.backbone.conv1.weight', 'anet.backbone.conv1.bias', 'anet.backbone.bn1.weight', 'anet.backbone.bn1.bias', 'anet.backbone.bn1.running_mean', 'anet.backbone.bn1.running_var', 'anet.backbone.bn1.num_batches_tracked', 'anet.backbone.layer1.0.bn1.weight']\n",
      "Found 5 images in /users/project1/pt01183/Building-height-width/external/neurvps/data/my5_sq\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/project1/pt01183/Building-height-width/external/neurvps/misc/run_on_folder.py\", line 208, in <module>\n",
      "    main(in_dir, out_dir, cfg, ckpt, device)\n",
      "  File \"/users/project1/pt01183/Building-height-width/external/neurvps/misc/run_on_folder.py\", line 195, in main\n",
      "    vps = infer_one(model, focal, device, p)\n",
      "  File \"/users/project1/pt01183/Building-height-width/external/neurvps/misc/run_on_folder.py\", line 175, in infer_one\n",
      "    pred = model(input_dict)   # <-- no focal kwarg here\n",
      "  File \"/mnt/host_scratch/envs/new_geospatial_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/host_scratch/envs/new_geospatial_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/users/project1/pt01183/Building-height-width/external/neurvps/neurvps/models/vanishing_net.py\", line 27, in forward\n",
      "    x = self.backbone(input_dict[\"image\"])[0]\n",
      "TypeError: 'dict' object is not callable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess\n",
    "\n",
    "REPO = \"/users/project1/pt01183/Building-height-width\"\n",
    "NEUR = f\"{REPO}/external/neurvps\"\n",
    "\n",
    "in_dir  = f\"{NEUR}/data/my5_sq\"\n",
    "out_dir = f\"{NEUR}/logs/tmm17/my5_vpts\"\n",
    "cfg_gsv = f\"{NEUR}/logs/tmm17/config_gsv.yaml\"\n",
    "ckpt = f\"{NEUR}/logs/tmm17/checkpoint_state_anetfix.pth\" # <-- use cleaned file\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = NEUR\n",
    "env.pop(\"TORCH_CUDA_ARCH_LIST\", None)\n",
    "\n",
    "cmd = [\n",
    "    \"/mnt/host_scratch/envs/new_geospatial_env/bin/python\",\n",
    "    f\"{NEUR}/misc/run_on_folder.py\",\n",
    "    in_dir, out_dir, cfg_gsv, ckpt, \"cuda:0\"\n",
    "]\n",
    "print(\"Running:\\n \", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, capture_output=True, text=True, env=env)\n",
    "print(res.stdout)\n",
    "print(res.stderr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4f3e8-945c-4b03-a5a3-2a9f3f6aa4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
